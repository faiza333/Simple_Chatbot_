# -*- coding: utf-8 -*-
"""Chatpot_Version_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v4_o3lCk_pB55yGXJTznr8YMHIJ8f98F
"""



import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
import json
import pickle

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from tensorflow.keras.optimizers import SGD
import random


from tqdm.autonotebook import tqdm

import nltk
nltk.download('omw-1.4')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import *

# from idlmam import*

from model import *

from google.colab import drive

drive.mount('/content/gdrive/', force_remount=True)

words=[]
classes = []
documents = []
ignore_words = ['?', '!']
data_file = open('/content/gdrive/MyDrive/intents.json').read()

intents = json.loads(data_file)

for intent in intents['intents']:
    for pattern in intent['patterns']:

        # take each word and tokenize it
        w = nltk.word_tokenize(pattern)
        words.extend(w)
        # adding documents
        documents.append((w, intent['tag']))

        # adding classes to our class list
        if intent['tag'] not in classes:
            classes.append(intent['tag'])

"""# Lemmatization
- lemmatization considers the context and converts the word to its meaningful base form

# Stemming
- stemming just removes the last few characters, often leading to incorrect meanings and spelling errors

- ‘Caring’ -> Lemmatization -> ‘Care’ ‘Caring’ -> Stemming -> ‘Car’
"""

words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]
words = sorted(list(set(words)))

classes = sorted(list(set(classes)))

print (len(documents), "documents")

print (len(classes), "classes", classes)

print (len(words), "unique lemmatized words", words)


pickle.dump(words,open('words.pkl','wb'))
pickle.dump(classes,open('classes.pkl','wb'))

# initializing training data
training = []
output_empty = [0] * len(classes)
# print(output_empty)
for doc in documents:
    # initializing bag of words
    bag = []
    # list of tokenized words for the pattern
    pattern_words = doc[0]
    # lemmatize each word - create base word, in attempt to represent related words
    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]
    # create our bag of words array with 1, if word match found in current pattern
    for w in words:
        bag.append(1) if w in pattern_words else bag.append(0)
        
    # output is a '0' for each tag and '1' for current tag (for each pattern)
    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1

    training.append([bag, output_row])
# shuffle our features and turn into np.array
random.shuffle(training)
training = np.array(training)
# create train and test lists. X - patterns, Y - intents
train_x = list(training[:,0])
train_y = list(training[:,1])
print("Training data created")

X_train = np.array(train_x)
y_train = np.array(train_y)

# # Train the model
# for epoch in range(num_epochs):
#     for (words, labels) in training_loader:
#         words = words.to(device)
#         labels = labels.to(dtype=torch.long).to(device)

#         # Forward pass
#         outputs = model(words)
#         # if y would be one-hot, we must apply
#         labels = torch.max(labels, 1)[1]
#         loss = criterion(outputs, labels)

#         # Backward and optimize
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()

#     if (epoch + 1) % 100 == 0:
#         print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')

# print(f'final loss: {loss.item():.4f}')

# data = {
# "model_state": model.state_dict(),
# "input_size": input_size,
# "hidden_size": hidden_size,
# "output_size": output_size,
# "all_words": words,
# "tags": classes
# }

# FILE = "data.pth"
# torch.save(data, FILE)

# print(f'training complete. file saved to {FILE}')


# def train_simple_network(model, loss_func, training_loader, epochs=20, device="cpu"):
#     #Yellow step is done here. We create the optimizer and move the model to the compute device
#     #SGD is Stochastic Gradient Decent over the parameters $\Theta$
#     optimizer = torch.optim.SGD(model.parameters(), lr=0.001)

#     #Place the model on the correct compute resource (CPU or GPU)
#     model.to(device)
#     #The next two for loops handle the Red steps, iterating through all the data (batches) multiple times (epochs)
#     for epoch in tqdm(range(num_epochs), desc="Epoch"):
    
#         model = model.train()#Put our model in training mode
#         running_loss = 0.0

#         for (words, labels) in tqdm(training_loader, desc="Batch", leave=False):
#             #Move the batch of data to the device we are using. this is the last red step
#             words = moveTo(words, device)
#             labels = moveTo(labels, device)

#             #First a yellow step, prepare the optimizer. Most PyTorch code will do this first to make sure everything is in a clean and ready state. 

#             #PyTorch stores gradients in a mutable data structure. So we need to set it to a clean state before we use it. 
#             #Otherwise, it will have old information from a previous iteration
#             optimizer.zero_grad()

#             #The next two lines of code perform the two blue steps
#             outputs = model(words) #this just computed $f_\theta(\boldsymbol{x_i})$
#             labels = torch.max(labels, 1)[1]
#             loss = criterion(outputs, labels)
#             # Compute loss.
#             loss = loss_func(y_hat, labels)

#             #Now the remaining two yellow steps, compute the gradient and ".step()" the optimizer
#             loss.backward()# $\nabla_\Theta$ just got computed by this one call

#             #Now we just need to update all the parameters
#             optimizer.step()# $\Theta_{k+1} = \Theta_k − \eta \cdot \nabla_\Theta \ell(\hat{y}, y)$

#             #Now we are just grabbing some information we would like to have
#             running_loss += loss.item()
#         if (epoch + 1) % 100 == 0:
#         print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')
# #Caption: This code defines a simple training loop, which can be used to learn the parameters $\Theta$ to almost any

# Hyper-parameters
num_epochs = 1000
batch_size = 8
learning_rate = 0.001
input_size = len(X_train[0])
hidden_size = 8
output_size = len(classes)
print(input_size, output_size)

class SimpleChatBotDataset(Dataset):
        
    def __init__(self):
        super(SimpleChatBotDataset, self).__init__()
        self.X = X_train
        self.y = y_train
        
    
    def __getitem__(self, index):
        return torch.tensor(self.X[index,:], dtype=torch.float32), torch.tensor(self.y[index], dtype=torch.float32)

    def __len__(self):
        return self.X.shape[0]
    
training_loader = DataLoader(SimpleChatBotDataset(), shuffle=True, num_workers=0,batch_size=batch_size)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = NeuralNet(input_size, hidden_size, output_size).to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# # Train the model
def train_simple_network(model, loss_func, training_loader, epochs=1000):
    for epoch in range(num_epochs):
        for (words, labels) in training_loader:
            words = words.to(device)
            labels = labels.to(dtype=torch.long).to(device)

            # Forward pass
            outputs = model(words)
            # if y would be one-hot, we must apply
            labels = torch.max(labels, 1)[1]
            loss = criterion(outputs, labels)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        if (epoch + 1) % 100 == 0:
            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')

    print(f'final loss: {loss.item():.4f}')

    data = {
    "model_state": model.state_dict(),
    "input_size": input_size,
    "hidden_size": hidden_size,
    "output_size": output_size,
    "all_words": words,
    "tags": classes
    }

    FILE = "data.pth"
    torch.save(data, FILE)

    print(f'training complete. file saved to {FILE}')

train_simple_network(model, criterion, training_loader, epochs=1000)

import random
import json

import torch


with open('/content/gdrive/MyDrive/intents.json', 'r') as json_data:
    intents = json.load(json_data)

FILE = "data.pth"
data = torch.load(FILE)

input_size = data["input_size"]
hidden_size = data["hidden_size"]
output_size = data["output_size"]
all_words = data['all_words']
tags = data['tags']
model_state = data["model_state"]

model = NeuralNet(input_size, hidden_size, output_size).to(device)
model.load_state_dict(model_state)
model.eval()

bot_name = "Sam"

words = pickle.load(open('words.pkl','rb'))
classes = pickle.load(open('classes.pkl','rb'))

from process import *

def get_response(msg):
    sentence =  tokenize(msg)
    X = bag_of_words(sentence, words)
    X = X.reshape(1, X.shape[0])
    X = torch.from_numpy(X).to(device)

    output = model(X)
    _, predicted = torch.max(output, dim=1)

    tag = classes[predicted.item()]

    probs = torch.softmax(output, dim=1)
    prob = probs[0][predicted.item()]
    if prob.item() > 0.75:
        for intent in intents['intents']:
            if tag == intent["tag"]:
                return random.choice(intent['responses'])

    return "I do not understand..."


if __name__ == "__main__":
    print("Let's chat! (type 'quit' to exit)")
    while True:
        # sentence = "do you use credit cards?"
        sentence = input("You: ")
        if sentence == "quit":
            break

        resp = get_response(sentence)
        print(resp)

